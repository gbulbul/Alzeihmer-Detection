# import system libs
import os
import time
import shutil
import pathlib
import itertools
import glob

# import data handling tools
import numpy as np
import pandas as pd
import seaborn as sns
sns.set_style('darkgrid')
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# import Deep learning Libraries
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam, Adamax
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation, Dropout, BatchNormalization
from tensorflow.keras import regularizers

# Ignore Warnings
import warnings
warnings.filterwarnings("ignore")
train = glob.glob("C:/Users/gbulb/Desktop/train/**/*.jpg")
test = glob.glob("C:/Users/gbulb/Desktop/test/**/*.jpg")
print("There is {} images in the training dataset".format(len(train)))
print("There is {} images in the test dataset".format(len(test)))
directory = "C:/Users/gbulb/Desktop/Alzheimer_s Dataset for detection"

datasets, Demented, Non_Demented  = ["train", "test"], [], []

for i in datasets:
    path = os.path.join(directory, i)
    MildDemented = glob.glob(os.path.join(path, "Demented/*.jpg"))

    NonDemented = glob.glob(os.path.join(path, "NonDemented/*.jpg"))

    Demented.extend(Demented),Non_Demented.extend(NonDemented)

print("The number of Demented images is: {}".format(len(Demented)))

print("The number of NonDemented images is: {}".format(len(Non_Demented)))

data = {
    'Category': ['Demented', 'NonDemented'],
    'Count': [len(Demented),len(Non_Demented)]
}

# Create a bar plot using Seaborn
sns.set(style="whitegrid")
plt.figure(figsize=(8, 6))
ax = sns.barplot(x='Category', y='Count', data=data, palette='viridis')
ax.set_title('Number of Images per Category')
plt.show()
# Image size
fftn = 224
fftm = 224

# Define the dataset directory and categories
dataset_dir = 'C:/Users/gbulb/Desktop/Alzheimer_s Dataset for detection'
categories = ['Demented', 'NonDemented']

# Load images and labels into separate lists
image_paths = []
labels = []

for category in categories:
    for split in ['train', 'test']:
        category_dir = os.path.join(dataset_dir, split, category)
        for image_name in os.listdir(category_dir):
            image_path = os.path.join(category_dir, image_name)
            image_paths.append(image_path)
            labels.append(category)
         
# Create a DataFrame with image paths and labels
data = pd.DataFrame({'image_path': image_paths, 'label': labels})

# Split the data into train and test DataFrames
train_df, test_df = train_test_split(data, test_size=0.2, stratify=data['label'], random_state=42)

# Reset the index of the DataFrames
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

# Display the first few rows of train_df and test_df
print(train_df.head())
print(test_df.head())
def show_images(gen):
    # Get the class names and a batch of images and labels from the generator
    classes = list(gen.class_indices.keys())
    images, labels = next(gen)
    
    # Calculate the number of images to display
    num_images = min(len(labels), 16)
    
    # Plot the images and their labels
    fig, axes = plt.subplots(nrows=4, ncols=4, figsize=(8, 8))
    for i, ax in enumerate(axes.flat):
        if i < num_images:
            ax.imshow(images[i])
            label_index = np.argmax(labels[i])
            ax.set_title(classes[label_index])
            ax.axis('off')
        else:
            ax.axis('off')
    
    plt.tight_layout()
    plt.show()
def plot_training(hist):
    tr_acc = hist.history['accuracy']
    tr_loss = hist.history['loss']
    val_acc = hist.history['val_accuracy']
    val_loss = hist.history['val_loss']
    index_loss = np.argmin(val_loss)
    val_lowest = val_loss[index_loss]
    index_acc = np.argmax(val_acc)
    acc_highest = val_acc[index_acc]

    plt.figure(figsize= (20, 8))
    plt.style.use('fivethirtyeight')
    Epochs = [i+1 for i in range(len(tr_acc))]
    loss_label = f'best epoch= {str(index_loss + 1)}'
    acc_label = f'best epoch= {str(index_acc + 1)}'
    plt.subplot(1, 2, 1)
    plt.plot(Epochs, tr_loss, 'r', label= 'Training loss')
    plt.plot(Epochs, val_loss, 'g', label= 'Validation loss')
    plt.scatter(index_loss + 1, val_lowest, s= 150, c= 'blue', label= loss_label)
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.subplot(1, 2, 2)
    plt.plot(Epochs, tr_acc, 'r', label= 'Training Accuracy')
    plt.plot(Epochs, val_acc, 'g', label= 'Validation Accuracy')
    plt.scatter(index_acc + 1 , acc_highest, s= 150, c= 'blue', label= acc_label)
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.tight_layout
    plt.show()
    
def plot_confusion_matrix(cm, classes, normalize= False, title= 'Confusion Matrix', cmap= plt.cm.Blues):
	plt.figure(figsize= (10, 10))
	plt.imshow(cm, interpolation= 'nearest', cmap= cmap)
	plt.title(title)
	plt.colorbar()
	tick_marks = np.arange(len(classes))
	plt.xticks(tick_marks, classes, rotation= 45)
	plt.yticks(tick_marks, classes)
	if normalize:
		cm = cm.astype('float') / cm.sum(axis= 1)[:, np.newaxis]
		print('Normalized Confusion Matrix')
	else:
         print('Confusion Matrix, Without Normalization')
	print(cm)
	thresh = cm.max() / 2.
	for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
		plt.text(j, i, cm[i, j], horizontalalignment= 'center', color= 'white' if cm[i, j] > thresh else 'black')
	plt.tight_layout()
	plt.ylabel('True Label')
	plt.xlabel('Predicted Label')
train_datagen = ImageDataGenerator(rescale=1./255,
    validation_split=0.2)

val_test_datagen = ImageDataGenerator(rescale=1./255)

# Define the batch size and target size
batch_size =16
target_size = (224, 224)

# Define the train and test generators
train_generator = train_datagen.flow_from_dataframe(
    dataframe=train_df,
    x_col='image_path',
    y_col='label',
    class_mode='categorical',
    shuffle=True,
    batch_size=batch_size,
    target_size=target_size,
    subset='training'
)

test_generator = val_test_datagen.flow_from_dataframe(
    dataframe=test_df,
    x_col='image_path',
    y_col='label',
    shuffle=False,
    class_mode='categorical',
    batch_size=batch_size,
    target_size=target_size
)

# Create Model Structure
img_size = (224, 224)
channels = 3
img_shape = (img_size[0], img_size[1], channels)
class_count = len(list(train_generator.class_indices.keys())) # to define number of classes in dense layer
early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)

####XCEPTION ##### Model 1
base_model = tf.keras.applications.Xception(include_top= False, weights= "imagenet", input_shape= img_shape, pooling= 'max',)

model = tf.keras.models.Sequential([
    base_model,
    Dense(512, activation = 'relu'),
    Dropout(0.2),
    Dense(1, activation = 'softmax')
])

model.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])
model.summary()


early_stopping_callbacks = tf.keras.callbacks.EarlyStopping(patience = 15, restore_best_weights = True, verbose = 1)

history = model.fit(train_generator,
    validation_data=test_generator,
    epochs=1,
    steps_per_epoch = 2,
    callbacks = [early_stopping_callbacks])
plot_training(history)
scores_model = []
scores_model.append({'Model': 'Xception','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

g_dict = test_generator.class_indices
classes = list(g_dict.keys())

# Confusion matrix
cm = confusion_matrix(test_generator.classes, y_pred)
plot_confusion_matrix(cm= cm, classes= classes, title = 'Confusion Matrix')

# Classification report
print(classification_report(test_generator.classes, y_pred, target_names= classes))

###############VGG############### Model 2
from keras.applications.vgg16 import VGG16, preprocess_input

base_model = VGG16(
    weights='imagenet',
    include_top=False, 
    input_shape=img_size + (3,)
)
class VGG16(Sequential):
    def __init__(self, params):
        super().__init__()

        self.add(base_model),
        self.add(layers.Flatten()),
        self.add(layers.Dropout(0.5)),
        self.add(layers.Dense(1, activation='sigmoid'))

        self.layers[0].trainable = False
        self.summary()
        self.compile(
    loss=params['loss'],
    optimizer=params['optimizer'],
    metrics=params['metric']
)
        
params={"input_shape":img_size,
        "loss":'binary_crossentropy',
        "optimizer":RMSprop(lr=1e-4),
        "metric": 'accuracy'}

model = VGG16(params)
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'VGG16','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)

################CancerNET############# Model 3

#from tensorflow.keras import backend as K
class CancerNet(Sequential):
    def __init__(self,params):
        super().__init__()
        self.add(SeparableConv2D(32, (3, 3), padding="same", input_shape=img_shape))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(SeparableConv2D(64, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(64, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(SeparableConv2D(128, (3, 3), padding="same"))
        self.add(Activation("relu"))
        self.add(BatchNormalization(axis=3))
        self.add(MaxPooling2D(pool_size=(2, 2)))
        self.add(Dropout(0.25))

        self.add(Flatten())
        self.add(Dense(256))
        self.add(Activation("relu"))
        self.add(BatchNormalization())
        self.add(Dropout(0.5))

        self.add(Dense(1))
        self.add(Activation("softmax"))
        self.summary()
        self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "img_shape":img_shape,
  "loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
model = CancerNet(params)
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'CancerNet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)


########################CNN###################### Model 4
class simple_CNN(Sequential):
    def __init__(self,params):
        super().__init__()
        self.add(layers.Conv2D(32, (3, 3), activation=params['activation'], input_shape= params["img_shape"]))
        self.add(layers.MaxPooling2D((2, 2)))
        self.add(layers.Conv2D(64, (3, 3), activation=params['activation']))
        self.add(layers.MaxPooling2D((2, 2)))
        self.add(layers.Conv2D(64, (3, 3), activation=params['activation']))
        self.add(layers.Flatten())
        self.add(layers.Dense(512, activation=params['activation'])),
        self.add(layers.Dense(1,activation=params['activation']))
        self.summary()
        self.compile(optimizer=params['optimizer'],
              loss=params['loss'],
              metrics=params['metric'])

params={
        "img_shape":img_shape,
  "loss": 'categorical_crossentropy',
"optimizer": "adam",
"activation": 'relu',
"activation_softmax":'softmax',
"batch_size": 128,
"metric":'accuracy',
}
model=simple_CNN(params)
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'CNN','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)


###########RESNET########## Model 5
from tensorflow.keras.applications import resnet_v2
transfer_net = resnet_v2.ResNet50V2(weights ='imagenet', include_top = False, input_shape = img_shape)
transfer_net.trainable = False
class ResNet(Sequential):
     def __init__(self,params):
         super().__init__()
         self.add(transfer_net)
         self.add(Flatten())
         self.add(Dense(512, activation = params['activation_relu']))
         self.add(Dropout(0.5))
         self.add(Dense(1, activation = params['activation_sigmoid']))
         self.summary()
         self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "img_shape":img_shape,
  "loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"activation_sigmoid": 'sigmoid',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
model=ResNet(params)
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'Resnet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)

preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)



##########ALEXNET#################### Model 6
class AlexNet(Sequential):
    def __init__(self, params):
        super().__init__()

        self.add(Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), padding="valid", activation="relu", input_shape=input_shape))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Conv2D(filters=256, kernel_size=(5, 5), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())

        self.add(Conv2D(filters=384, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())

        self.add(Conv2D(filters=256, kernel_size=(3, 3), strides=(1, 1), padding="same", activation="relu"))
        self.add(BatchNormalization())
        self.add(MaxPooling2D(pool_size=(3, 3), strides=(2, 2)))

        self.add(Flatten())
        self.add(Dense(4096, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(4096, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(256, activation="relu"))
        self.add(Dropout(0.5))
        self.add(BatchNormalization())

        self.add(Dense(1, activation="softmax"))

        self.summary()
        self.compile(optimizer = Adam(params["learning_rate"]), loss =params["loss"], metrics = params["metric"])
params={
        "input_shape":img_shape,
  "loss": 'binary_crossentropy',
"optimizer": "adam",
"activation_relu": 'relu',
"activation_sigmoid": 'sigmoid',
"learning_rate": 0.00001,
"batch_size": 128,
"metric":'accuracy',
}
model = AlexNet(params)
def fitting(train_generator, test_generator):
    history = model.fit(train_data=train_generator, validation_data=test_generator, epochs=10, callbacks = [early_stopping_callbacks] )
    return history
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'AlexNet ','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)


######EfficientNet############### Model 7
base_model = tf.keras.applications.efficientnet.EfficientNetB0(include_top= False, weights= "imagenet", input_shape= img_shape, pooling= 'max')
# base_model.trainable = False
class EfficientNet(Sequential):
    def __init__(self, input_shape):
        super().__init__()

        self.add(base_model),
        self.add(BatchNormalization(axis= -1, momentum= 0.99, epsilon= 0.001)),
        self.add(Dense(256, kernel_regularizer= regularizers.l2(l= 0.016), activity_regularizer= regularizers.l1(0.006),
                bias_regularizer= regularizers.l1(0.006), activation= 'relu')),
        self.add(Dropout(rate= 0.45, seed= 123)),
        self.add(Dense(1, activation= 'softmax'))
        self.summary()
        self.compile(Adamax(learning_rate= 0.001), loss= 'categorical_crossentropy', metrics= ['accuracy'])

model = EfficientNet(img_shape)
history=fitting(train_generator, test_generator)
scores_model.append({'Model': 'EfficientNet','accuracy': history.history['val_accuracy'] ,"loss": history.history['val_loss']})
print(scores_model)
preds = model.predict_generator(test_generator)
y_pred = np.argmax(preds, axis=1)
